<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <meta content="description" name="description">
    <meta name="google" content="notranslate"/>
    <meta content="Mashup templates have been developped by Orson.io team" name="author">

    <!-- Disable tap highlight on IE -->
    <meta name="msapplication-tap-highlight" content="no">

    <link rel="robot-icon" sizes="180x180" href="./assets/robot-icon-180x180.png">
    <link href="./assets/robot-icon-180x180.ico" rel="icon">

    <title>Tech Watch</title>

    <link href="./main.3f6952e4.css" rel="stylesheet">
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
    <nav class="navbar  navbar-fixed-top navbar-default">
        <div class="container">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbar-collapse">
                <ul class="nav navbar-nav ">
                    <li><a href="./index.html" title="">Home</a></li>
                    <li><a href="./intro.html" title="">Introduction</a></li>
                    <li><a href="./article1.html" title="">GANs</a></li>
                    <li><a href="./article2.html" title="">Beautiful Art vs Terrifying Deepfakes</a></li>
                    <li><a href="./article3.html" title="">What's next ?</a></li>
                    <li><a href="./news.html" title="">News</a></li>
                    <li><a href="./videos.html" title="">Videos</a></li>
                    <li><a href="./about.html" title="">About</a></li>
                </ul>

            </div>
        </div>
    </nav>
</header>
<div class="section-container">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
                <div class="card-container">
                    <div class="text-center">
                        <h1 class="h2">Introduction</h1>
                    </div>
                    <p style="text-align:center; font-size:120%;">Cet article est une introduction à l'intelligence
                        artificielle et
                        ses récentes avancées</p>

                    <blockquote>
                        <p>“GANs are the coolest idea in deep learning in the last 10 years.”</p>
                        <small class="pull-right">Yann LeCun</small>
                    </blockquote>
                </div>
            </div>


            <div class="col-md-8 col-md-offset-2 section-container-spacer">
                <div class="row">
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/work000-02.jpg" class="img-responsive" alt="">
                        <p>Ian Goodfellow - Inventeur des GANs</p>
                    </div>
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/work000-03.jpg" class="img-responsive" alt="">
                        <p>Les machines pourront-elles un jour penser ?</p>
                    </div>
                </div>
            </div>

            <div class="col-xs-12">

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Bien que le terme intelligence artificielle soit né en 1950 dans un article publié par Alan Turing
                    « Computing Machinery and Intelligence », sa mise en pratique et les avancées technologiques qui y
                    sont liées n'auront jamais été aussi performantes que depuis ces dernières années.
                    Néanmoins, l’IA en tant que domaine scientifique existe depuis 1956 suite à la conférence
                    tenue au Darmouth College, ce qui sonne le début de
                    <a href="https://siecledigital.fr/2018/08/20/histoire-intelligence-artificielle/">l’histoire de
                        l’intelligence artificielle</a>. Aujourd’hui, elle vient de pair avec d’autres termes comme
                    algorithme, machine learning, réseau de neurones artificiels ou deep learning notamment. Voyons
                    ensemble ce qui hiérarchise tout cela.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Tout d’abord, sous le terme intelligence artificielle (IA) on regroupe l’ensemble des « théories et
                    des techniques mises en œuvre en vue de réaliser des machines capables de simuler l’intelligence ».
                    On distingue l’IA faible de l’IA forte. Par définition, l’IA faible est un programme qui n’est pas
                    doté de sens et se concentre uniquement sur la tâche pour laquelle il a été programmé.
                    On l'oppose à l’IA forte, qui, elle, est dotée de conscience et donc d’une sensibilité
                    particulière. Cependant, l’IA forte reste encore du domaine de la science-fiction.
                    Toutes les intelligences artificielles que nous croisons dans la réalité sont faibles.
                    Ce sont les technologies capables d'accomplir des tâches spécifiques aussi bien, voire mieux, que
                    nous, les humains. Des exemples d'intelligence artificielle faible sont la classification d'images
                    ou la reconnaissance faciale. Ces technologies présentent certaines
                    facettes de l'intelligence humaine. Mais comment ? D'où vient cette intelligence ? Cela nous amène au terme
                    suivant, le machine learning ou apprentissage automatique.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    L'apprentissage automatique dans sa forme la plus élémentaire est l'utilisation d'algorithmes pour
                    analyser les données,
                    en tirer des leçons et ensuite faire une prédiction concernant une tâche particulière. Ainsi, plutôt
                    que de coder à
                    la main des routines logicielles avec un ensemble spécifique d'instructions pour accomplir une tâche
                    particulière,
                    la machine est « entraînée » en utilisant de grandes quantités de données et d'algorithmes qui lui
                    donnent la
                    capacité d'apprendre comment exécuter la tâche. Le véritable sacre de l’apprentissage automatique
                    survient
                    lorsqu’en 1997 Deep Blue bat Garry Kasparov aux échecs, invaincu jusqu’alors. Le superordinateur
                    créé par IBM
                    inspirera la naissance de Watson, mais aussi de nombreux projets d’apprentissage des intelligences
                    artificielles.
                    Le développement rapide de l’information, du traitement des données, et du cloud nous emmènera
                    jusqu’à AlphaGo
                    développé par DeepMind, rachetée par Google en 2014. Aujourd’hui, le machine learning continue
                    d’être utilisé,
                    mais plus dans le cas de systèmes experts, ou de superordinateurs. Ces techniques ont posé les bases
                    permettant
                    de faire émerger des intelligences artificielles plus complexes, creusant plus profond. C’est le cas
                    des réseaux
                    de neurones artificiels.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Si le terme « réseau de neurones » apparaît depuis peu dans les médias, c’est qu’il était très
                    difficile à mettre en place jusqu’à récemment.
                    En effet, tenter de faire raisonner un programme comme raisonne le cerveau humain nécessite une puissance
                    informatique conséquente. C’est pour cette raison que les théories étaient fréquentes, mais
                    aucune application ne voyait le jour. Pourtant cette approche est née avec les premières réflexions
                    sur l’intelligence artificielle dans les années 1950. Pour comprendre le fonctionnement d’un réseau
                    de neurones (artificiel), il faut imaginer plusieurs couches, avec des connexions entre elles.
                    Chaque couche peut correspondre à une tâche, et chaque neurone
                    a une mission bien précise. Au départ, le réseau se trompe, beaucoup, vraiment beaucoup. À force de
                    confronter ses résultats avec la réalité, à force de voir différentes images, il apprend.
                    C’est à partir de ce fonctionnement, et grâce, encore une fois, au développement des capacités de calcul
                    des ordinateurs, qu’est né le deep learning.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    En effet, c’est en 2012 que l’IA explose véritablement. À ce moment-là, Nvidia sort sur le marché
                    ses processeurs graphiques (GPU) ultra-performants. A l’époque, pour Andrew Ng, cofondateur de Google Brain,
                    l’exploitation de ces GPU pourrait créer
                    des systèmes d’apprentissage profond jusqu’à 100 fois plus rapides. L’entraînement des algorithmes
                    passerait de
                    plusieurs semaines à seulement quelques jours. Ce sera bien le cas et la vaste disponibilité des GPU
                    permettra de
                    créer des traitements parallèles pour les programmes et ainsi les rendre plus rapides, moins chers,
                    et même plus
                    puissants. Ajoutons à cela, depuis 2015, la quantité astronomique de données que l’on peut utiliser
                    et stocker,
                    et les intelligences artificielles ont depuis tout pour s’épanouir.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    De nombreuses théories d’apprentissage, les structures de réseaux de neurones artificiels,
                    les applications de certains traitements ont vu le jour dès les premiers instants de l’intelligence artificielle.
                    Cependant, l’informatique aura été la clé de son épanouissement et de sa mise en pratique. De l’intelligence
                    artificielle basique au deep learning, il aura fallu attendre des décennies. Cependant au vu des récentes
                    avancées l’attente en aura valu la peine.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p align="center">
                    <img src="./assets/images/IA.jpg" class="img-responsive" alt="" align="middle">
                </p>
                <p></p>
                <div style="height:30px;font-size:20px;">&nbsp;</div>


                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Il y a quelques années, Ian Goodfellow étudiait comment les réseaux de neurones peuvent apprendre
                    sans supervision humaine.
                    Généralement, un réseau a besoin d’exemples étiquetés par un label pour apprendre efficacement. Il
                    est également possible
                    d’apprendre à partir de données non étiquetées, mais cela n’a généralement pas très bien fonctionné.
                    L’idée principale
                    était de créer de nouvelles données plausibles sans effort d’étiquetage, mais les résultats
                    n'étaient souvent pas très
                    bons : les images d'un visage généré par ordinateur avaient tendance à être floues ou comportaient
                    des erreurs telles
                    que des oreilles manquantes.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Un soir de 2014, suite à un débat avec des amis à lui dans un pub de Montréal, Ian Goodfellow eu
                    l’idée d’appliquer la théorie
                    des jeux entre deux réseaux de neurones en les opposant afin de générer des images plausibles. Un
                    des réseaux apprendrait
                    la distribution des données fournies dans le dataset et générerait ainsi des exemples similaires. Le
                    second essayerait de
                    distinguer les exemples générés des exemples réels permettant au premier de modifier ses paramètres
                    dans un effort
                    d’amélioration. Ce qu'il a inventé cette nuit-là s'appelle désormais un GAN, à savoir Generative
                    Adversarial Network ou réseau antagoniste
                    génératif. Cette technique a suscité un immense enthousiasme dans le domaine de l'apprentissage
                    automatique et a
                    transformé son créateur en une célébrité de l'IA.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Au cours des dernières années, les chercheurs en intelligence artificielle ont réalisé des progrès
                    impressionnants en
                    utilisant une technique appelée apprentissage profond ou deep learning. En fournissant à un système
                    d’apprentissage
                    profond suffisamment d’images il sera capable d’apprendre, par exemple, à reconnaître un piéton qui
                    est sur le point
                    de traverser une route. Cette approche a rendu possibles des avancées comme les voitures autonomes
                    et la technologie
                    de conversation qui alimente Alexa, Siri et d’autres assistants virtuels.
                    Mais si ce type d’IA, à savoir des réseaux de neurones profonds, peuvent apprendre à reconnaître des
                    choses, elles n’ont pas
                    été douées pour les créer. Le but des GANs est de donner aux machines quelque chose que nous
                    pourrions assimiler à une
                    forme d'imagination.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p align="center">
                    <img src="./assets/images/music.jpg" class="img-responsive" alt="" align="middle">
                </p>
                <div style="height:30px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Au-delà de simplement dessiner de jolis dessins ou de composer de la musique, cela rendrait ces
                    algorithmes moins dépendants
                    des humains pour les renseigner sur le monde et son fonctionnement. Aujourd’hui, les programmeurs de
                    systèmes
                    d’intelligence artificielle doivent souvent indiquer de façon exacte à une machine ce que
                    contiennent les données
                    d’entraînement dont elle est « nourrie » - parmi ce million d’images, lesquelles contiennent un
                    piéton traversant
                    une route et lesquelles ne le font pas. En plus d’être coûteux et de demander une force de travail
                    conséquente,
                    cela limite la capacité du système à gérer même de légers écarts par rapport à ce sur quoi il a été
                    formé : le
                    système manquera de robustesse. À l'avenir, grâce à des technologies comme les GANs, les ordinateurs
                    seront bien
                    mieux capables d’ingérer des données brutes et de déterminer d’eux-mêmes ce dont ils ont besoin
                    d’apprendre de ces
                    données sans (presque) aucune indication extérieure.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Cela marquera une grande avancée dans ce que l’intelligence artificielle appelle « l'apprentissage
                    non supervisé ».
                    Une voiture autonome pourrait par exemple apprendre par elle-même diverses conditions de circulation
                    sans même
                    quitter son garage.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Notre capacité d’imagination et de réflexion sur de nombreux scénarios différents les uns des autres
                    est en partie ce qui fait de nous des humains. Certains voient les GANs comme un grand pas en avant
                    dans la création de machines avec un semblant de conscience humaine ; bien qu'on en soit encore très loin.
                    De grands noms dans le domaine de l'IA en ont également parlé avec beaucoup d'enthousiasme.
                    Yann LeCun, scientifique en chef de l'IA chez Facebook, a qualifié les GANs « d'idée la plus cool en
                    matière
                    d'apprentissage profond au cours des dix dernières années ». Andrew Ng, l'ancien scientifique en
                    chef du chinois
                    Baidu, a déclaré que les GANs représentaient « une avancée significative et fondamentale » qui a
                    inspiré une
                    communauté mondiale croissante de chercheurs.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    L'engouement vis-à-vis de la technologie est justifié. Les GANs ont obtenu des résultats
                    remarquables, longtemps
                    impossibles pour des systèmes artificiels, tels que la capacité de générer des images réalistes ou
                    de transformer
                    une séquence vidéo d'un cheval en mouvement en un zèbre, le tout sans la nécessité de disposer de
                    données
                    d'entraînement soigneusement étiquetées. Contrairement à d’autres progrès de l’apprentissage
                    automatique,
                    le succès des GANs a dépassé le simple monde de la recherche et a su attirer l’attention du grand
                    public,
                    là où d’autres technologies ne suscitaient auparavant qu’un simple regard interrogateur. En effet,
                    les réseaux
                    génératifs ont été mis en avant par le New York Times, la BBC, Scientific American et de nombreux
                    autres médias
                    de premier plan. Autre témoignage de l’attrait de cette technologie,
                    <a href="https://www.ladn.eu/tech-a-suivre/art-et-intelligence-artificielle-decouvrez-le-collectif-obvious/">
                        un portrait réalisé par un GAN
                    </a>
                    a récemment été vendu aux enchères de Christie’s pour plus de 430 000 dollars.</p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Concernant le monde de la recherche, depuis que Goodfellow et quelques autres ont publié la première
                    étude sur sa
                    découverte, en 2014, des centaines d'articles sur le GAN ont été écrits. Un fan de la technologie a
                    même créé
                    une page web appelée <a href="https://deephunt.in/the-gan-zoo-79597dc8c347">« GAN Zoo »</a>, dédiée
                    au suivi des différentes
                    versions développées et basées sur la technique. En février 2018, le site recensait plus de 500
                    articles
                    scientifiques majoritairement écrits au cours des deux dernières années.</p>


                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p align="center">
                    <img src="./assets/images/gan-paper.jpg" class="img-responsive" alt="" align="middle" id="paper">
                </p>
                <p></p>
                <div style="height:30px;font-size:20px;">&nbsp;</div>

            </div>

        </div>
    </div>
</div>


<footer class="footer-container text-center">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <p></p>
            </div>
        </div>
    </div>
</footer>

<script>
    document.addEventListener("DOMContentLoaded", function (event) {
        navActivePage();
    });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

-->
<script type="text/javascript" src="./main.70a66962.js"></script>
</body>

</html>>